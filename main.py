import psycopg2
from dotenv import load_dotenv
import os
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import logging
import uvicorn
from transformers import CLIPVisionModelWithProjection, CLIPTokenizer, CLIPTextModelWithProjection
import torch
from core.event_loader import event_loader
from core.image_utils import (
    preprocess_image, 
    get_image_embedding, 
    calculate_image_demo_similarity
)
from core.onnx_utils import load_onnx_vision_model
from typing import Dict, Optional
import time

# í”„ë ˆì„ ê°„ ìœ ì‚¬ë„ ë³€í™” ì¶”ì ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
previous_similarities: Dict[str, float] = {}  # {query_label: previous_similarity}
frame_count: Dict[str, int] = {}  # {query_label: frame_count}
event_active: Dict[str, bool] = {}

# init models
model_name = "Searchium-ai/clip4clip-webvid150k" #"taett/omni" 
t_model = CLIPTextModelWithProjection.from_pretrained(model_name)
tokenizer = CLIPTokenizer.from_pretrained(model_name)
# v_model = CLIPVisionModelWithProjection.from_pretrained(model_name)
# v_model = v_model.eval()

# ONNX ë¹„ì „ ëª¨ë¸ ë¡œë“œ
# onnx_model_path = "./core/onnx_vision"
onnx_model_path = "./onnx_vision_re_re/model.onnx"
v_model = load_onnx_vision_model(onnx_model_path)

# GPU ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPUë¡œ ì´ë™ (CUDA ë˜ëŠ” MPS)
if torch.cuda.is_available():
    t_model = t_model.cuda()
    v_model = v_model.cuda()
    print("Models loaded on CUDA GPU")
elif torch.backends.mps.is_available():
    t_model = t_model.to('mps')
    # v_model = v_model.to('mps')
    print("Models loaded on MPS (Apple Silicon GPU)")
else:
    print("Models loaded on CPU")

print("Models initialized successfully")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = FastAPI(title="OMNi VLM Inference API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class InferenceResponse(BaseModel):
    success: bool
    event_detected: bool
    event_active: bool
    similarity_score: float
    query_label: str
    query_text: str
    message: str
    threshold: float  # ê¸°ë³¸ ì„ê³„ê°’

# Load environment variables from .env
load_dotenv()

# Fetch variables
USER = os.getenv("user")
PASSWORD = os.getenv("password")
HOST = os.getenv("host")
PORT = os.getenv("port")
DBNAME = os.getenv("dbname")

# Connect to the database
try:
    connection = psycopg2.connect(
        user=USER,
        password=PASSWORD,
        host=HOST,
        port=PORT,
        dbname=DBNAME
    )
    print("Connection successful!")
    
    # Create a cursor to execute SQL queries
    cursor = connection.cursor()
    
    # Example query
    cursor.execute("SELECT NOW();")
    result = cursor.fetchone()
    print("Current Time:", result)

    # Close the cursor and connection
    cursor.close()
    connection.close()
    print("Connection closed.")

except Exception as e:
    print(f"Failed to connect: {e}")



@app.post("/api/vlm/inference", response_model=InferenceResponse)
async def vlm_inference(
    image: UploadFile = File(..., description="ì›¹ìº  ìº¡ì²˜ ì´ë¯¸ì§€ (JPEG)"),
    query: str = Form(..., description="ì‚¬ìš©ìê°€ ì„ íƒí•œ ì´ë²¤íŠ¸ì˜ ì˜ì–´ ì„¤ëª…"),
    query_label: str = Form(..., description="ì‚¬ìš©ìê°€ ì„ íƒí•œ ì´ë²¤íŠ¸ì˜ í•œêµ­ì–´ ë¼ë²¨")
):
    """
    VLM ëª¨ë¸ì„ í™œìš©í•œ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ê°ì§€
    
    Args:
        image: ì›¹ìº  ìº¡ì²˜ ì´ë¯¸ì§€ (JPEG Blob)
        query: ì‚¬ìš©ìê°€ ì„ íƒí•œ ì´ë²¤íŠ¸ì˜ ì˜ì–´ ì„¤ëª… (ì˜ˆ: "Someone is making a V-shape with two fingers")
        query_label: ì‚¬ìš©ìê°€ ì„ íƒí•œ ì´ë²¤íŠ¸ì˜ í•œêµ­ì–´ ë¼ë²¨ (ì˜ˆ: "ë¸Œì´ í•˜ê¸°")
    
    Returns:
        InferenceResponse: ì´ë²¤íŠ¸ ê°ì§€ ê²°ê³¼
    """
    # í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
    start_time = time.time()
    
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ê²€ì¦
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ë°”ì´íŠ¸ ì½ê¸°
        image_bytes = await image.read()
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_tensor = preprocess_image(image_bytes)
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ
        image_embedding = get_image_embedding(image_tensor, v_model)
        
        # ë°ëª¨ ì´ë²¤íŠ¸ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        demo_embedding = event_loader.get_embedding_by_query_label(query_label)
        demo_similarity = 0.0
        
        if demo_embedding is not None:
            demo_similarity = calculate_image_demo_similarity(image_embedding, demo_embedding)
        else:
            logger.warning(f"Demo embedding not found for query_label: {query_label}")
        
        current_similarity = demo_similarity
        
        # í”„ë ˆì„ ì¹´ìš´íŠ¸ ì¦ê°€
        if query_label not in frame_count:
            frame_count[query_label] = 0
        frame_count[query_label] += 1
        
        # ì•ŒëŒ íŒë‹¨ ë¡œì§
        
        # ì„ê³„ê°’ ì •ì˜
        similarity_threshold = 0.2   # ìœ ì§€ ì—¬ë¶€ íŒë‹¨ (ë¸Œì´ ê³„ì† ìœ ì§€ë˜ëŠ” ë™ì•ˆ ê°ì§€ ON)
        gap_threshold = 0.05         # ìˆœê°„ ë³€í™” íŒë‹¨ (ìƒˆ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°)


        alert = False
        event_status = "none"     # active / none
        previous_similarity = previous_similarities.get(query_label, 0.0)
        similarity_gap = current_similarity - previous_similarity
        
        # ì²« í”„ë ˆì„ì€ íŒë‹¨ ìŠ¤í‚µ
        if frame_count[query_label] == 1:
            logger.info(f"First frame for {query_label}: Skipping alert")
            alert = False
        else:
            # 1. gapê³¼ similarity ëª¨ë‘ ë§Œì¡± â†’ ìƒˆ ì´ë²¤íŠ¸ ë°œìƒ
            if (similarity_gap >= gap_threshold 
                and current_similarity >= similarity_threshold 
                and not event_active.get(query_label, False)):   # ì•„ì§ í™œì„±í™” ì•ˆëœ ê²½ìš°ë§Œ
                alert = True
                event_status = "active"
                event_active[query_label] = True
                logger.info(f"New event triggered for {query_label}")

            # 2. ì´ë¯¸ ë°œìƒí•œ ì´ë²¤íŠ¸ ìœ ì§€ ì¤‘
            elif event_active.get(query_label, False):
                if current_similarity >= similarity_threshold:
                    alert = False                 # ìœ ì§€ ì¤‘ì—” ì•Œë¦¼ ì°ì§€ ë§ê³ 
                    event_status = "active"       # ongoing ëŒ€ì‹  active
                    logger.info(f"Event still active for {query_label}")
                else:
                    event_active[query_label] = False
                    event_status = "none"
                    logger.info(f"Event ended for {query_label}")
        
        # í˜„ì¬ ìœ ì‚¬ë„ë¥¼ ë‹¤ìŒ í”„ë ˆì„ì„ ìœ„í•´ ì €ì¥
        previous_similarities[query_label] = current_similarity
        
        # í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ ë° ì¶œë ¥
        end_time = time.time()
        execution_time = end_time - start_time
        
        # ë¡œê¹…
        logger.info(f"Query: {query}, Label: {query_label}")
        logger.info(f"Frame: {frame_count[query_label]}, Similarity: {current_similarity:.4f}, Alert: {alert}")
        logger.info(f"vlm_inference ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ")
        
        # í„°ë¯¸ë„ì— ì‹¤í–‰ ì‹œê°„ ì¶œë ¥
        print(f"ğŸš€ vlm_inference í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ (Query: {query_label})")
        
        # return InferenceResponse(
        #     similarity=current_similarity,
        #     event=query_label,
        #     alert=alert
        # )
        return InferenceResponse(
            success=True,
            event_detected=alert, # bool
            event_active=event_active.get(query_label, False),
            similarity_score=current_similarity,
            query_label=query_label,
            query_text=query,
            message=(
        "ì´ë²¤íŠ¸ ìƒˆë¡œ ê°ì§€ë¨" if alert 
        else ("ì´ë²¤íŠ¸ ìœ ì§€ ì¤‘" if event_active.get(query_label, False) else "ì´ë²¤íŠ¸ ê°ì§€ë˜ì§€ ì•ŠìŒ")),
            threshold=similarity_threshold
        )

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        end_time = time.time()
        execution_time = end_time - start_time
        print(f"âŒ vlm_inference í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ (ì—ëŸ¬ ë°œìƒ)")
        
        logger.error(f"Error in VLM inference: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì´ë²¤íŠ¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/api/vlm/events")
async def get_available_events():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ ëª©ë¡ì„ ë°˜í™˜"""
    try:
        events = event_loader.get_available_events()
        return {
            "success": True,
            "events": events,
            "count": len(events)
        }
    except Exception as e:
        logger.error(f"Error getting available events: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/api/vlm/reset")
async def reset_frame_state():
    """í”„ë ˆì„ ìƒíƒœ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘ ì‹œ ì‚¬ìš©)"""
    global previous_similarities, frame_count
    previous_similarities.clear()
    frame_count.clear()
    
    logger.info("Frame state reset - starting new session")
    return {
        "success": True,
        "message": "Frame state reset successfully"
    }

@app.get("/api/vlm/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    # GPU ìƒíƒœ í™•ì¸
    gpu_info = {
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available(),
        "device_type": "unknown"
    }
    
    if torch.cuda.is_available():
        gpu_info["device_type"] = "cuda"
        gpu_info["cuda_device_count"] = torch.cuda.device_count()
    elif torch.backends.mps.is_available():
        gpu_info["device_type"] = "mps"
    else:
        gpu_info["device_type"] = "cpu"
    
    return {
        "status": "healthy",
        "models_loaded": True,
        "gpu_info": gpu_info,
        "available_events": len(event_loader.get_available_events()),
        "active_sessions": len(frame_count)
    } 

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

        