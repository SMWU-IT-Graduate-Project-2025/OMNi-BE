#!/usr/bin/env python3
"""
OMNi VLM Inference API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” êµ¬í˜„ëœ FastAPI ì„œë²„ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì˜ˆì œì…ë‹ˆë‹¤.
"""

import requests
import json
from pathlib import Path

# API ì„œë²„ URL
BASE_URL = "http://localhost:8000"

def test_health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    print("=== ì„œë²„ ìƒíƒœ í™•ì¸ ===")
    try:
        response = requests.get(f"{BASE_URL}/api/vlm/health")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… ì„œë²„ ìƒíƒœ: {data['status']}")
            print(f"âœ… ëª¨ë¸ ë¡œë“œë¨: {data['models_loaded']}")
            
            # GPU ì •ë³´ í‘œì‹œ
            gpu_info = data.get('gpu_info', {})
            device_type = gpu_info.get('device_type', 'unknown')
            print(f"âœ… ë””ë°”ì´ìŠ¤ íƒ€ì…: {device_type}")
            
            if device_type == 'cuda':
                print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {gpu_info.get('cuda_available', False)}")
                print(f"âœ… CUDA ë””ë°”ì´ìŠ¤ ìˆ˜: {gpu_info.get('cuda_device_count', 0)}")
            elif device_type == 'mps':
                print(f"âœ… MPS ì‚¬ìš© ê°€ëŠ¥: {gpu_info.get('mps_available', False)}")
            else:
                print("â„¹ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
            
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ ìˆ˜: {data['available_events']}")
            return True
        else:
            print(f"âŒ ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_get_events():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ"""
    print("\n=== ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ ëª©ë¡ ===")
    try:
        response = requests.get(f"{BASE_URL}/api/vlm/events")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… ì´ë²¤íŠ¸ ìˆ˜: {data['count']}")
            print("ğŸ“‹ ì´ë²¤íŠ¸ ëª©ë¡:")
            for event in data['events']:
                print(f"  - {event}")
            return data['events']
        else:
            print(f"âŒ ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

def test_reset():
    """í”„ë ˆì„ ìƒíƒœ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\n=== í”„ë ˆì„ ìƒíƒœ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ===")
    try:
        response = requests.post(f"{BASE_URL}/api/vlm/reset")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… ì´ˆê¸°í™” ì„±ê³µ: {data['success']}")
            print(f"âœ… ë©”ì‹œì§€: {data['message']}")
            return True
        else:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ OMNi VLM Inference API í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # 1. ì„œë²„ ìƒíƒœ í™•ì¸
    if not test_health_check():
        print("âŒ ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”.")
        return
    
    # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ
    events = test_get_events()
    if not events:
        print("âŒ ì´ë²¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. í”„ë ˆì„ ìƒíƒœ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    test_reset()
    
    # 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ì´ë²¤íŠ¸ ê°ì§€ í…ŒìŠ¤íŠ¸
    test_image_path = "test_image.jpg"  # í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ê²½ë¡œ
    if Path(test_image_path).exists():
        # ì˜ˆì œ ì¿¼ë¦¬ë“¤
        test_cases = [
            {
                "query": "Someone is making a V-shape with two fingers",
                "query_label": "ë¸Œì´ í•˜ê¸°"
            },
            {
                "query": "A person is waving his five fingers",
                "query_label": "ì•ˆë…•"
            },
            {
                "query": "A person is giving a thumbs-up sign",
                "query_label": "ì—„ì§€ì²™"
            },
            {
                "query": "Usual store conditions",
                "query_label": "ì •ìƒ"
            }
        ]
        
        for test_case in test_cases:
            test_inference(
                test_image_path,
                test_case["query"],
                test_case["query_label"]
            )
    else:
        print(f"âš ï¸  í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {test_image_path}")
        print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
